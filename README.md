# Image-Captioning

Image Captioning is the task of translating an input image into a textual description. As such, it connects Vision and Language in a generative fashion, with applications that range from multi-modal search engines to help visually impaired people.

Dataset used : https://www.kaggle.com/datasets/adityajn105/flickr8k

Data split : Training data = 6000 Images,  Testing data = 1000 Images, Validation data = 1000 Images

# Model Architechture
![image](https://github.com/vansh-py04/Image-Captioning/assets/128248352/21824ccf-0962-4505-ac50-f0eef2a72cf9)

# Model Weight losses
![Screenshot 2024-04-07 003549](https://github.com/vansh-py04/Image-Captioning/assets/128248352/01913f46-dbe8-49f4-88d8-6c9b6ff66ee5)
Based on this, model_19 was used for making predictions

# Predictions 
![image](https://github.com/vansh-py04/Image-Captioning/assets/128248352/165219b1-e267-460d-bcb9-db6cce62e724)

![image](https://github.com/vansh-py04/Image-Captioning/assets/128248352/53a2c2c9-d515-426f-b8ae-084f71ab3733)
